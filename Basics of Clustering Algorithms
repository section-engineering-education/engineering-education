# Basics of Clustering Algorithms 

When presented with a plethora of scattered data points, it can prove helpful to find the trends and similarities within the randomness that is data collection. However, it will prove to be a daunting challenge on how to analyze large scatters of points. The process of testing every single data point as valid to a group or cluster will prove to be tedious and time-consuming, especially when there are hundreds of data points to verify. Thankfully, clustering is a widely utilized tool in unsupervised learning[https://towardsdatascience.com/unsupervised-learning-and-data-clustering-eeecb78b422a] algorithms to speed up the organization without any human input. Taking a process that could have taken hundreds of man-hours and reducing it to short computing time. 

## What is clustering?

Clustering is the act of gathering data points and assorting these points into sectors based on similarities. In the more popular forms, cluster algorithms determine the similarities of data points by varying degrees of distance between one point and another. Computationally, the proximity of data points is typically calculated using Euclidean distances based functions or similar derivations. The algorithm processes the data array and builds a structure from the determined similarities. Depending on the structure of the cluster, it may be optimal to choose one type of cluster algorithm [https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html#:~:text=The%20centers%20of%20clusters%20should,the%20dataset%20and%20every%20cluster.] over another; doing this may reduce the processing time or increase the accuracy. To increase understanding of these organization types we will cover two basic types of cluster algorithms that are popularly used across the industry.

## Basic types of Clustering Algorithms

### Centroid based

This type of algorithm identifies a cluster of data according to the closeness of a determined focal point. The algorithm finds an optimum center in a group and does an iterative method of calculations to process the validity of these clusters. Something note of this method is this requires prior knowledge of the number of clusters in the data set. For example, in the infographic below, it would be stated there are three known sets of clusters in the data set and separate these accordingly.
![](https://drive.google.com/file/d/1rjfbLUzpBhtL-G_Eeb3gxtmAfbhdvvIh/view?usp=sharing)
One of the best examples of this algorithm is known as K-means. Where “k” is defined as the total number of determined center points. K-means is a widely utilized resource due to its easy implementation, relatively quick processing speed, scalability, consistent converges, and adaptability to new data sets. Although K-mean offers simplicity it does not handle outliers well, requires a manual k value, produces varying results from the same information, and does not easily identify clusters of varying density and dimensions.  K-means may be used in areas where there is linear information, such as determining test scores or the probability of having heart attacks.

### Connectivity based

As can be guessed, this model based on the Euclidean distance of data points. It classifies the data points close in proximity have more similarity than data points that are wildly spaced apart. Connectivity can be broken down into 2 different approaches: top-down and bottom-up.

The bottom-up method treats every data point as a single cluster and merges each cluster from the increasing distance till all the points are contained within a single cluster; much like mitosis performed in reverse. Top-down approaches use the same method in reverse, where all data points start as a single cluster and divided into independent clusters varying on the subjective distance of the points. These algometric methods are typically visualized through a hierarchical dendrogram chart much like the infographic below.  
![](https://drive.google.com/file/d/1BoTMCw4htlMt6U27tp7MmcluK4hZRiDe/view?usp=sharing)
One of the main examples of this algorithm is known as hierarchal clustering. Unlike K-means, hierarchical does not require prior knowledge of the total number of cluster points and functions quadratically rather than a linear. Given that it has increased complexity it also requires more processing power and offers low efficiency. That being said, it will have reproducible results, can work in multiple dimensions, is not sensitive to the distance metric, and has the unique feature of being able to recover parts of the hierarchy. This type of algorithm can be useful areas such as identifying different classes of plants and animals through similarities in DNA, predicting the stock market, or even determining different classes of cells.
